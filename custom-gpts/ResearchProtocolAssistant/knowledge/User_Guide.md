# Multi-Paper Research Synthesis with Deep Research and Knowledge Graphs

This guide outlines a general-purpose process for using two Deep Research protocols in tandem to synthesize multiple research papers and enrich the results with knowledge graphs. We use examples from LAMM Lab’s PRefLexOR and SciAgents (“SPARKS”) papers as a running case study to illustrate each step. The process is modular and extensible, meaning you can plug in future steps (deeper fusion, formal reasoning, planning, etc.) as needed.&#x20;

## Protocol Roles and Overview

### 1) SIO JSON-LD Ontology Extractor

This protocol produces two artifacts from one or more target papers: (1) a query-oriented RDF knowledge graph  and (2) a high-quality Markdown reference report. The objective is not to summarize the papers. Instead, the system expands on their content by running targeted web research to surface related methods, datasets, replications, critiques, benchmarks, and implementation resources.

Single-paper mode: Given a paper and its extracted RDF graph, the system generates a reference report that ties the paper’s content to external evidence relevant to the user’s query. The report organizes this information into a structured Markdown document with in-line, verifiable citations.

Multi-paper mode: The system can also ingest multiple papers, each paired with its corresponding RDF graph, and produce a detailed comparative report across all of them. In this mode, it highlights similarities, differences, methodological overlaps, conflicting results, and external context, while also suggesting merged ontology maps for downstream reasoning.

The RDF graphs encode entities, claims, metrics, and relationships—each with provenance—so questions can be answered directly via graph traversal or SPARQL, with traceable links back to both the source papers and external materials.

Example: Given the query “How does Graph-PRefLexOR compare to Zep Graphiti for reasoning over heterogeneous knowledge graphs?”, the system can process both papers and their graphs, retrieve recent evaluations from the web, and generate a report that lays out side-by-side comparisons, caveats, and references to code and datasets. The RDF graphs instantiate nodes for methods, datasets, tasks, metrics, and outcomes, plus cross-paper edges such as evaluated_on, outperforms_on, and contested_by, enabling precise, provenance-aware answers across the full set of inputs.

### 2) Comprehensive Reference Report Generation

This protocol produces a high-quality Markdown reference report from a paper and an RDF graph generated by the SIO JSON-LD Ontology Extractor. It digests the paper’s content into a structured report with section headings, bullet points, and in-line citations. The report highlights key findings, methods, and references from the paper, functioning as a detailed yet readable overview for humans.

**Example:** Running the report generator on a paper like Graph-PRefLexOR yields a well-organized expanded report of the model’s approach (graph-based reasoning with recursive refinement) and its results, complete with citations to the original paper’s sections or figures.


Together, these two protocols give complementary views of each paper: a human-friendly report and a machine-readable knowledge graph. The report helps interpret and verify the content, while the graph formalizes the content for computational linking and reasoning. Recent research projects (like LAMM’s) show the power of combining natural language and knowledge graphs – for instance, the SciAgents system uses “large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts”, alongside LLMs and agents \[7]. Similarly, Graph-PRefLexOR explicitly frames reasoning as a mapping from a question to a knowledge graph of concepts and relations, which enables hierarchical inference and uncovering hidden connections \[8]. These examples underscore why a dual approach can yield deeper insights than text or graphs alone.


## Workflow for Multi-Paper Knowledge Graph Synthesis

### 1) Collect and Prepare Documents

Gather the research papers you want to synthesize. If they are in PDF format, convert each PDF to Markdown (or plain text) first. This step is optional but highly recommended – having the paper in Markdown improves the consistency of text formatting and makes it easier for the protocols to parse content (headings, lists, etc.). For instance, converting the SciAgents (2024) PDF to Markdown ensures section titles like “Results” or “Case Studies” are clearly demarcated, and references are easy to identify in text.

### 2) Generate a Knowledge Graph for Each Paper

Run the Ontology Extractor on the Markdown text of each paper to produce an RDF knowledge graph (in JSON-LD format). Each paper will yield its own graph (think of this as **Paper Knowledge Graph (PKG)**). Key points:

* **One Node per Key Entity:** The extractor will identify important entities such as methods, materials, datasets, metrics, hypotheses, etc., and create a node for each \[9]. Every node is typed (classified) using ontology terms (e.g., an algorithm might be typed as `sio:Process`, a material as `sio:Material`, a numerical result as `sio:Metric`) and given a unique IRI. For example, in the Graph-PRefLexOR paper’s graph, there might be a node `GraphPReflexorModel` of type `sio:Process` with a label “Graph-PRefLexOR framework”, and another node `ThinPlacesConcept` of type `sio:Concept`.

* **Relation Assertions:** The extractor will also create relations (edges) between nodes based on statements in the text \[10]. For instance, from a sentence “Graph-PRefLexOR combines graph reasoning with symbolic abstraction to dynamically expand domain knowledge” \[11], it might create triples like:

  ```
  GraphPReflexorModel — sio:hasPart     → GraphReasoningProcess
  GraphPReflexorModel — sio:hasProperty → SymbolicAbstraction
  GraphPReflexorModel — sio:achieves    → KnowledgeExpansion
  ```

  Each relation in JSON-LD includes a predicate, object, and a `text_span` showing evidence \[10]. The provenance of that assertion (source paper DOI, timestamp, model used) is attached as well \[4].

* **Axioms (Optional):** If the paper’s text hints at general rules or class hierarchies, the extractor may include an **axioms** section with OWL-like statements \[12]. For example, if multiple papers indicate that “all graph reasoning models are a type of AI reasoning process”, an axiom could state that the class `GraphReasoningModel` is a subclass of `AIProcess`. Axioms help when merging graphs later, but they can be skipped initially if uncertain.

After this step, you will have one JSON-LD graph per paper. Save these (or note where they are stored) for analysis and merging. In our case study, we’d have one graph for **SciAgents** and one for **Graph-PRefLexOR**, each capturing the key concepts (nodes like “ontological knowledge graph”, “multi-agent system”, “hypothesis generation”, etc. for SciAgents; and “graph reasoning”, “knowledge garden growth”, “thin places”, etc. for PRefLexOR) and their interrelations as described in each paper.

### 3) Generate a Comprehensive Markdown Report for Each Paper

Next, run the **Comprehensive Reference Report Generation** protocol on each paper’s text (again, using the Markdown version of the paper if available). This will produce a Markdown reference report that reads like a mini-review or detailed review of the paper. Each report typically includes:

* **Introduction and Background:** Summaries of the problem domain and objectives of the paper. For example, the report for SciAgents would introduce the challenge of autonomous scientific discovery and the need to combine knowledge graphs with LLMs and agents \[7].
* **Methods/Approach:** Key methods and the framework proposed. E.g., for PRefLexOR, the report would describe how Graph-PRefLexOR “combines graph reasoning with symbolic abstraction” in a recursive loop \[11]. It would mention that reasoning in this framework is represented as a knowledge graph of concepts and relations, with references to the paper’s sections.
* **Results and Findings:** Important results, often as bullet points for clarity. For instance, it might list that Graph-PRefLexOR demonstrated superior reasoning depth compared to standard LLMs, or that it could creatively link domains (like mythological “thin places” to material science concepts) \[13]. For SciAgents, it would note discoveries of hidden interdisciplinary links and successful autonomous hypothesis generation \[14].
* **Figures or Examples:** The report may describe key figures or examples from the paper. If a paper included a diagram of its knowledge graph or agent architecture, the report might summarize what that figure shows (e.g., “Figure 1 illustrates the iterative graph reasoning loop: the model generates reasoning tokens, extracts a local graph, merges it into a global graph, and formulates a new query for the next iteration” \[15]).
* **Citations:** In-line citations (using the Deep Research `【†】` format) point to the source material. These let you verify facts or dive deeper into the original text easily.

Each Markdown report serves as a human-readable companion to the raw JSON-LD graph. At this stage, you have for each paper: (a) a structured graph file, and (b) a comprehensive report in Markdown.

### 4) Human-in-the-Loop Graph Inspection and Refinement

This step is crucial for quality control and deeper understanding. Engage in a chat with ChatGPT (or another assistant) using the extracted graph as context, to inspect and refine it. The goal is to improve the graph’s accuracy and completeness with a human expert in the loop, guided by AI suggestions. Here’s how to proceed:

* **Load the Graph:** Provide the JSON-LD content to the chat and ask the assistant to interpret it. For example, “Here is an RDF graph extracted from Paper X. Can you summarize what’s captured in this graph?” This prompt yields a high-level summary of the nodes and relations, helping you verify if the graph “makes sense” relative to the paper. It should correctly identify the major entities and how they relate.
* **Query the Graph for Missing Info:** Ask targeted questions. For instance:

  * “What axioms are present or missing in this graph?” – This prompt has the assistant examine any axioms in the JSON-LD (like subclass relations or restrictions). The assistant might say, for example, “The graph defines Graph-PReflexOR as a type of Process, but it doesn’t explicitly state that Graph-PReflexOR is a subclass of a broader AI Model class – you might consider adding that axiom if relevant.” If important relationships (like subclass-of or part-of hierarchies) are missing, note them down.
  * “Are there any key concepts from the paper that are not represented in the graph?” – This helps find omissions. If the assistant knows (from the text or the earlier summary) that a concept like “swarm intelligence” was central in SciAgents, but there’s no node for it, this is a candidate to add.
  * “Can you propose higher-level conceptual links among these nodes?” – This prompt encourages the assistant to suggest abstract connections. For example, it might observe that several nodes relate to materials design and propose a grouping or a super-class for them, or suggest that Agent and LLM nodes could both be linked under a concept of AI components. These suggestions can inspire adding new edges or nodes that integrate the knowledge graph.
* **Iterative Refinement:** Based on the chat insights, you (the human researcher) can decide to refine the graph. This might involve editing the JSON-LD:

  * Merging duplicate nodes (if the extractor made separate nodes that actually refer to the same concept).
  * Adding missing nodes or edges that were obvious to you but not picked up automatically.
  * Adding an `owl:Class` axiom or `subClassOf` relation to better categorize a set of entities (for example, tagging all Instrument entities as subclasses of `sio:Instrument` if that wasn’t explicit).
  * Changing any incorrect types or relations (perhaps the extractor mis-typed a Process as an Attribute, etc., which you can fix).
* **Validation:** You can ask the assistant to validate the changes. For example, after edits, “Here is the revised graph. Summarize the main differences or improvements compared to the previous version.” This ensures your refinements had the intended effect.

This human-in-the-loop process leverages the assistant’s natural language understanding to debug and enrich the knowledge graph. It combines the precision of automated extraction with the intuition of a human expert. In our case study, this step might reveal, say, that the SciAgents graph is missing a direct link indicating that “knowledge graph reasoning leads to hypothesis generation”, or that in PRefLexOR’s graph, an axiom could be added to declare `GraphReasoning` as a subclass of a more general `ReasoningProcess`. We can then incorporate those refinements.

### 5) Optional: Reprocess or Cross-Verify the Reports

After refining the graph, you have new insights about the paper’s knowledge structure. You may choose to loop back to the textual report to update or verify it, though this is optional. There are a couple of ways to do this:

* **Targeted Deep Research Q\&A:** Use Deep Research capabilities in a Q\&A style to answer new questions that arose from the graph. For example, if the graph refinement highlighted a relationship not clearly described before, you could ask: “According to the paper, how does concept X relate to concept Y?” and use the Deep Research system to retrieve an answer with citations from the text. This ensures your understanding (and the written report) include the connections now evident in the graph.
* **Update the Report with New Findings:** If significant new connections or interpretations emerged (say you realized two papers use different terms for the same concept, or one paper’s result can inform another’s approach), you can append a section in the Markdown report about “Graph-Based Insights” or “Post-Extraction Notes”. Here you might write, for example: “Through the ontology extraction, we identified that Paper A’s ‘dynamic knowledge graph’ plays a similar role to Paper B’s ‘knowledge garden growth’ concept, suggesting a common principle of iterative knowledge expansion.” This kind of note links the papers at a conceptual level and can be cited back to both original sources.
* **Quality Check:** Rerun the Comprehensive Report protocol on the original text, but with instructions to incorporate certain details. For instance, you might feed a prompt like: “Regenerate the summary, ensuring you mention the concept of ‘knowledge garden growth’ and its role, which was identified in the ontology.” The protocol can then produce a refined report that includes that detail, properly cited.

This step ensures that the narrative (the Markdown report) and the structure (the knowledge graph) are aligned and both as complete as possible. It’s a feedback loop between unstructured and structured representations of the knowledge.

### 6) Repeat for All Papers in the Set

Perform Steps 2–5 for each paper in your research set. By the end, you will have a collection of refined per-paper graphs and corresponding summary reports. In our example, after processing both SciAgents and Graph-PRefLexOR papers, we have two graphs (each capturing that paper’s insights) and two detailed summaries. Throughout, we might have noted common themes or complementary pieces (e.g., both papers emphasize modularity in their AI systems \[16], or one provides details that fill gaps in the other).

### 7) Aggregate Knowledge Graphs and Reports

Finally, aggregate the knowledge from all processed papers to enable cross-paper synthesis and downstream applications:

* **Merge Graphs:** Combine the individual JSON-LD graphs into one integrated knowledge graph. This can be as simple as concatenating the node lists if there are no overlaps, or a more sophisticated merge where you unify identical concepts. For example, if Paper A and Paper B both discuss “ontological knowledge graph” as a concept, ensure this is one node in the merged graph (linking back to both sources via provenance). Use consistent IRIs or `owl:sameAs` links for identical nodes across papers. The result is a meta-knowledge graph representing the union of insights from all documents. This graph can be saved in a triplestore or any RDF database for querying.
* **Integrated Report or Dashboard:** You can also create a top-level report that summarizes the collection of papers. This could be a Markdown document that outlines the common findings, differing points, and an overall narrative of the field. It might reference each paper’s report (or even include sections from them). Given the graphs, you might include a section like “Key Concepts Across Papers” listing major recurring concepts and how each paper contributes to them. For instance, “Both studies highlight the importance of iterative reasoning: one via a swarm of agents updating a knowledge graph \[14], the other via a single model growing a ‘knowledge garden’ graph \[13]. This suggests a convergent insight that iterative knowledge graph expansion is beneficial for discovery.” Such an integrated summary is valuable for readers or for informing further analysis.

At this point, you have a rich synthesis: human-readable reports and a machine-readable knowledge network that spans multiple sources. This dual artifact can now be leveraged in various powerful ways.

## Downstream Applications of the Synthesized Knowledge

By combining deep textual understanding with structured knowledge, you unlock several advanced applications:

* **Automated Episode Planning:** The integrated knowledge graph can serve as a memory or world model for planning algorithms or agent-based systems. For example, an AI agent could traverse the graph to plan a sequence of reasoning “episodes” or tasks. (In the SciAgents project, a multi-agent system uses an ontological graph to decide which hypotheses or experiments to pursue next \[17].) Similarly, you could use the graph to script an “episode” for a simulation or a story, ensuring the plot follows logically from the scientific knowledge captured.
* **Experimental Design and Hypothesis Generation:** Researchers can query the combined graph to identify gaps or novel connections that suggest experiments. E.g., if one part of the graph links material **A** to property **X** and another links property **X** to a process **B**, the graph might suggest investigating whether material **A** can be used in process **B**. The structured representation makes it easier to apply logical rules or semantic queries (SPARQL) to find such patterns. This supports hypothesis generation and experiment planning, effectively automating part of the scientific method. Indeed, the SciAgents system autonomously “generates and refines research hypotheses” by leveraging its knowledge graph \[14] – you could do the same in a controlled, human-in-the-loop fashion with your merged graph.
* **Cross-Paper Question Answering:** With all information in a unified graph, you can build QA systems that answer questions using evidence from multiple papers. The Deep Research reports provide detailed citations for textual evidence, and the graph provides a conceptual map for reasoning. A question like “How do knowledge graphs improve AI-driven materials discovery?” could be answered by combining insights from both example papers (with references to each). The graph might help by directly pointing to the node **KnowledgeGraph** and its relations to outcomes like hypothesis generation or interdisciplinary link discovery, which you can then articulate in natural language with the help of the reports.
* **Dynamic App or Agent Generation:** The knowledge graph can be programmatically converted into applications. For instance, one could generate an interactive knowledge explorer app where users click on nodes to see the supporting literature (using the stored text spans and report snippets). Or create a chatbot agent whose knowledge is grounded in these papers – it can answer queries or make recommendations, constrained by the graph’s factual content. If you have a multi-agent framework (like the SPARKS model from LAMM Lab), the agents could use the graph as a shared memory to coordinate (one agent could focus on one subgraph, another on a different aspect, etc., collaborating to solve a problem).
* **Educational Tools:** Transform the integrated report and graph into a tutorial or teaching material. The structured nature allows generating mind-maps or concept graphs for students, and the text provides explanations. A student could follow the graph to see how concepts interrelate, and read the attached summaries for understanding – effectively learning a mini-domain synthesized from multiple sources.

These are just a few possibilities. The key is that by having information in both readable narrative form and structured knowledge form, you can drive applications that require understanding, reasoning, and even creativity, in a transparent and verifiable way.

## Best Practices for Graph Refinement Prompts

During the graph refinement stage (Step 4 above), the prompts you use when conversing with the assistant make a big difference. Here are some effective prompt types and how to use them:

* **“Summarize what’s in this graph.”** – Use this early to get a quick overview. It prompts the AI to read the JSON-LD and describe the main entities and relationships. This helps ensure the graph captures the paper’s essence. For example, it might respond: “The graph contains nodes for ‘LLM’, ‘Knowledge Graph’, ‘Multi-Agent System’, and shows relationships like the LLM and the multi-agent system both utilizing the knowledge graph for discovery.” Such a summary lets you catch obvious errors or omissions right away.
* **“What axioms are present or missing?”** – This is a pointed way to have the assistant inspect the ontology aspects of the graph. If the graph includes an axioms section (for subclasses or properties), the assistant will list them (e.g., “It defines GraphReasoningModel as a subclass of AIProcess”). More importantly, it can suggest missing axioms: “The graph doesn’t specify that ‘Experiment’ is a type of Process – if that’s implied in the paper, you might add that.” This ensures your knowledge graph isn’t just a flat list of facts, but has some of the hierarchical structure that experts would expect.
* **“Can you propose higher-level conceptual links?”** – After verifying the basics, this prompt taps into the AI’s ability to generalize. It might identify that several nodes relate to a broader category. For instance: “Several nodes involve ‘material properties’ and ‘biological analogies’. Perhaps introduce a higher-level node ‘Bio-Inspired Design’ to group those.” Or “Paper A and Paper B both mention ‘iterative reasoning’ – maybe link their processes under a common node.” These conceptual links help in fusing the graphs from multiple papers and can inspire how you merge them in step 7. They effectively propose an ontological schema that spans the documents.
* **“Explain node \[X] in context.”** – If you see a node and aren’t sure why it’s there or what it fully means, ask the assistant to explain it using the text evidence. E.g., “Explain the node ‘knowledge garden growth’ in context – what does it refer to in the paper?” This will yield a brief definition or description, often pulling from the `rdfs:comment` or `text_span`. It verifies that each node is meaningful. If the explanation is unclear or trivial, you may need to refine that node (e.g., add a better comment or merge it with another node).
* **“Check consistency between text and graph.”** – You can provide a specific part of the paper (or refer to the Markdown report) and ask if the graph reflects it. For example, “The paper’s conclusion says the approach outperformed baselines in X and Y – do we have nodes/relations that capture those outcomes?” This can highlight missing result nodes or important comparative assertions to add to the graph.

When using these prompts, it’s good practice to copy relevant parts of the JSON-LD or the paper text into the conversation (within limits) so the assistant has the necessary context. Always instruct the assistant to focus on the graph content and the paper’s content – this keeps the refinement grounded in facts and avoids speculative additions.

## Modularity and Extensibility of the Process

One strength of this approach is its modular design. You can customize or extend each part without breaking the whole workflow:

* **Modular Inputs:** If you already have structured data (e.g., an existing ontology or database related to the papers), you can plug that into the process. For example, if an external ontology for “materials science” exists, you might link the extracted graph nodes to classes in that ontology (using `owl:equivalentClass` or similar) during refinement.
* **Swappable Components:** Each protocol works independently – you might use a different summarization tool in place of the Deep Research report generator, or a different extraction tool for the graph. As long as the outputs (Markdown text and JSON-LD graph) are produced, the later steps remain the same. This means the process can adapt to new state-of-the-art tools or specific domain tools.
* **Feedback Loops:** The optional reprocessing step (Step 5) highlights that this isn’t a strict linear pipeline. You can loop back or iterate any stage. You might even do multiple rounds of extraction: run the ontology extractor, refine the graph, then run the extractor again on the same text with updated context or on the combined corpus of all papers to get a graph that spans papers. The process supports such iterative deepening.
* **Future Extensions:** After creating and merging the knowledge graphs, you can plug in deeper fusion or reasoning modules. For instance, you could use an OWL reasoner on the merged graph to infer new relationships (e.g., if **A** `subclass-of` **B** and **B** `subclass-of` **C**, infer **A** `subclass-of` **C**). Or apply graph algorithms to find central nodes or communities across the papers (as was done in one study with a 1,000-paper graph to find interdisciplinary links \[18]). Furthermore, planning algorithms or multi-agent systems (like those in LAMM Lab’s work) can be layered on top: the graph becomes a live knowledge base that agents can query and update as they simulate research tasks.
* **Scalability:** Each paper can be processed in parallel, and graphs merged gradually. If a new paper in the domain is published, you can run it through the same protocols and then integrate it into the existing knowledge graph without starting from scratch.

By keeping the steps modular, you ensure that improvements or changes in one step (say a more accurate extractor, or an updated ontology) can be incorporated without reworking everything. The synergy of a narrative report and an ontology graph remains the core, and additional steps (like those for validation, reasoning, or application-building) can attach naturally. In essence, you’re building a pipeline that mimics a human researcher’s workflow (reading, summarizing, conceptualizing, connecting ideas) but with the consistency and power of machine assistance.

---

Using the Deep Research **Comprehensive Report Generation** and **SIO JSON-LD Ontology Extractor** together offers a powerful approach to literature review and knowledge synthesis. In our example, they allowed us to digest complex papers on AI-driven scientific discovery into understandable summaries and a connected web of concepts. The knowledge graphs provided a way to see the common threads and unique contributions of each work (e.g., how both Preflexor and SciAgents emphasize iterative knowledge expansion \[14]\[13], albeit in different contexts), while the reports ensured we didn’t lose the nuance and evidence from the original texts. By following this guide, researchers and developers can similarly tackle a new set of papers, confident that they’ll end up with both a comprehensive reference and a knowledge graph — a combination that not only aids understanding, but also powers advanced applications in AI, automation, and beyond.

## References

* **\[1] \[2] \[3] \[4] \[5] \[6] \[9] \[10] \[12]** *Deep\_Research\_SIO\_JSON-LD\_Ontology\_Extractor.md*
  file://file-P1Sy1WLPGBZyeAgUGES4Zj

* **\[7] \[14] \[16]** *Paper page — SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning*
  [https://huggingface.co/papers/2409.05556](https://huggingface.co/papers/2409.05556)

* **\[8] \[13]** *Graph-PRefLexOR and the Dawn of Interpretable AI Reasoning | Joshua Berkowitz*
  [https://joshuaberkowitz.us/blog/research-reviews-2/graph-preflexor-and-the-dawn-of-interpretable-ai-reasoning-457](https://joshuaberkowitz.us/blog/research-reviews-2/graph-preflexor-and-the-dawn-of-interpretable-ai-reasoning-457)

* **\[11]** *In-situ graph reasoning and knowledge expansion using Graph-PReFLexOR (arXiv:2501.08120)*
  [https://arxiv.org/abs/2501.08120](https://arxiv.org/abs/2501.08120)

* **\[15]** *Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks*
  [https://arxiv.org/html/2502.13025v1](https://arxiv.org/html/2502.13025v1)

* **\[17]** *SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning (arXiv:2409.05556)*
  [https://arxiv.org/abs/2409.05556](https://arxiv.org/abs/2409.05556)

* **\[18]** *Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning (arXiv:2403.11996)*
  [https://arxiv.org/abs/2403.11996](https://arxiv.org/abs/2403.11996)
